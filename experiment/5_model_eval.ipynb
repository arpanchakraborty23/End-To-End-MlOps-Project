{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\MLOPS\\\\End-To-End-MlOps-Project'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI'] = os.getenv('MLFLOW_TRACKING_URI')\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = os.getenv('MLFLOW_TRACKING_USERNAME')\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = os.getenv('MLFLOW_TRACKING_PASSWORD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelEvalConfig:\n",
    "   dir: Path\n",
    "   test_arr:Path\n",
    "   all_params: str\n",
    "   model: Path\n",
    "   metrics: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dsproject.utils.utils import create_dir,read_yaml,save_json,load_obj\n",
    "from src.dsproject.constants.yaml_path import *\n",
    "from src.dsproject import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigManager:\n",
    "    def __init__(self,config_file_path=Config_file_path,prams_file_path=Param_file_path,schema_file_path=Schema_file_path) -> None:\n",
    "        self.config=read_yaml(config_file_path)\n",
    "        self.params=read_yaml(prams_file_path)\n",
    "        self.schema=read_yaml(schema_file_path)\n",
    "\n",
    "        create_dir([self.config.artifacts_root])\n",
    "    def get_model_eval_config(self):\n",
    "        try:\n",
    "         config=self.config.Model_Eval\n",
    "         create_dir([config.dir])\n",
    "         schema=self.schema\n",
    "         params=self.params.RandomForestClassifier\n",
    "         model_eval_config=ModelEvalConfig(\n",
    "                  dir=config.dir,\n",
    "                  test_arr=config.test_arr,\n",
    "                  all_params=params,\n",
    "                  model=config.model,\n",
    "                  metrics=config.metrics\n",
    "               )\n",
    "         return model_eval_config\n",
    "\n",
    "         \n",
    "        except Exception as e:\n",
    "         logging.info(f'error in MODEL eval config {str(e)}')\n",
    "         raise e\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,classification_report\n",
    "import pickle\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEval:\n",
    "    def __init__(self, config:ModelEvalConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def eval_metrics(self, y_actual, y_pred):\n",
    "        try:\n",
    "            accuracy = accuracy_score(y_actual, y_pred) * 100\n",
    "            precision = precision_score(y_actual, y_pred) * 100\n",
    "            recall = recall_score(y_actual, y_pred) * 100\n",
    "            clf_report=classification_report(y_actual,y_pred)\n",
    "\n",
    "            return accuracy, precision, recall,clf_report\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info(f'Error in Model evaluation metrics: {str(e)}')\n",
    "            raise e\n",
    "\n",
    "    def initiating_model_eval(self):\n",
    "        try:\n",
    "            logging.info('Model evaluation started')\n",
    "\n",
    "            # Load test data and model\n",
    "            test_arr = np.load(self.config.test_arr,allow_pickle=True)\n",
    "            model = load_obj(self.config.model)\n",
    "\n",
    "            x_test = test_arr[:, :-1]\n",
    "            y_test = test_arr[:, -1]\n",
    "\n",
    "            # Set MLflow tracking URI\n",
    "            mlflow.set_tracking_uri(os.getenv('MLFLOW_TRACKING_URI'))\n",
    "\n",
    "            # Infer the signature for the model\n",
    "            sig = infer_signature(x_test, y_test)\n",
    "\n",
    "            # Start MLflow run for logging\n",
    "            with mlflow.start_run():\n",
    "                # Make predictions\n",
    "                predict = model.predict(x_test)\n",
    "\n",
    "                # Calculate evaluation metrics (accuracy, precision, recall)\n",
    "                accuracy, precision, recall,clf_report = self.eval_metrics(y_actual=y_test, y_pred=predict)\n",
    "                scores = {'accuracy': accuracy, 'precision': precision, 'recall': recall,\"classification report\":clf_report}\n",
    "\n",
    "                # Save metrics as JSON\n",
    "               \n",
    "                save_json(filename=self.config.metrics, data=scores)\n",
    "\t\t\t\t\t #Log params\n",
    "                mlflow.log_params(self.config.all_params)\n",
    "                # Log metrics to MLflow\n",
    "                mlflow.log_metric('Accuracy', accuracy)\n",
    "                mlflow.log_metric('Precision', precision)\n",
    "                mlflow.log_metric('Recall', recall)\n",
    "\n",
    "                mlflow.log_text(clf_report,'classsification_report.txt')\n",
    "\n",
    "                # Log the model with signature\n",
    "                tracking_uri_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "                if tracking_uri_type_store != 'file':\n",
    "                    mlflow.sklearn.log_model(model, 'model', registered_model_name='best_model')\n",
    "                else:\n",
    "                    mlflow.sklearn.log_model(model, 'model', signature=sig)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info(f'Error in Model evaluation: {e}')\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-06 18:56:58,750: INFO :utils : Yaml file config\\config.yaml created]\n",
      "[2024-11-06 18:56:58,755: INFO :utils : Yaml file params.yaml created]\n",
      "[2024-11-06 18:56:58,761: INFO :utils : Yaml file schema.yaml created]\n",
      "[2024-11-06 18:56:58,764: INFO :utils : created directory at: artifacts]\n",
      "[2024-11-06 18:56:58,767: INFO :utils : created directory at: artifacts/model_eval]\n",
      "[2024-11-06 18:56:58,770: INFO :4179958558 : Model evaluation started]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\MLOPS\\End-To-End-MlOps-Project\\ds\\Lib\\site-packages\\_distutils_hack\\__init__.py:16: UserWarning: Distutils was imported before Setuptools, but importing Setuptools also replaces the `distutils` module in `sys.modules`. This may lead to undesirable behaviors or errors. To avoid these issues, avoid using distutils directly, ensure that setuptools is installed in the traditional way (e.g. not an editable install), and/or make sure that setuptools is always imported before distutils.\n",
      "  warnings.warn(\n",
      "d:\\MLOPS\\End-To-End-MlOps-Project\\ds\\Lib\\site-packages\\_distutils_hack\\__init__.py:31: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "Registered model 'best_model' already exists. Creating a new version of this model...\n",
      "2024/11/06 18:57:26 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: best_model, version 4\n",
      "Created version '4' of model 'best_model'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "            config=ConfigManager()\n",
    "            model_eval_config=config.get_model_eval_config()\n",
    "            model_eval=ModelEval(model_eval_config)\n",
    "            model_eval.initiating_model_eval()\n",
    "\n",
    "except Exception as e:\n",
    "            logging.info(f'Error occured {str(e)}')\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
